{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da50d4b8-7f69-44b4-a296-eed99f9c9883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark object: <pyspark.sql.connect.session.SparkSession object at 0x7efc9b3519a0>\nspark.version: 4.0.0\n+---+\n|  n|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sanity check 1: is spark session available?\n",
    "try:\n",
    "    print(\"spark object:\", spark)\n",
    "    print(\"spark.version:\", spark.version)\n",
    "    # tiny Spark job to see core functionality\n",
    "    spark.range(5).toDF(\"n\").show()\n",
    "except Exception as e:\n",
    "    print(\"ERROR running spark test:\", type(e).__name__, str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87aaa0bb-8f99-4276-af52-139acc22e797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (1.5.3)\nCollecting openpyxl\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pyarrow in /databricks/python3/lib/python3.12/site-packages (15.0.2)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.12/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nCollecting et-xmlfile (from openpyxl)\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\nDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install pandas openpyxl pyarrow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c657a2ff-b5a5-4f74-9b50-96e4f8541d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas dataframe shape: (7, 6)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>name</th>\n",
       "      <th>department</th>\n",
       "      <th>salary</th>\n",
       "      <th>joining_year</th>\n",
       "      <th>bonus_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Alice</td>\n",
       "      <td>HR</td>\n",
       "      <td>50000</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Finance</td>\n",
       "      <td>60000</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>IT</td>\n",
       "      <td>75000</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>David</td>\n",
       "      <td>Finance</td>\n",
       "      <td>62000</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Eva</td>\n",
       "      <td>IT</td>\n",
       "      <td>80000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>Frank</td>\n",
       "      <td>HR</td>\n",
       "      <td>52000</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>Grace</td>\n",
       "      <td>IT</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id     name department  salary  joining_year  bonus_percent\n",
       "0          101    Alice         HR   50000          2018              5\n",
       "1          102      Bob    Finance   60000          2019              7\n",
       "2          103  Charlie         IT   75000          2017             10\n",
       "3          104    David    Finance   62000          2020              6\n",
       "4          105      Eva         IT   80000          2021             12\n",
       "5          106    Frank         HR   52000          2018              5\n",
       "6          107    Grace         IT   90000          2019             15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark schema:\nroot\n |-- employee_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- joining_year: long (nullable = true)\n |-- bonus_percent: long (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: bigint, name: string, department: string, salary: bigint, joining_year: bigint, bonus_percent: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL A - read Excel into pandas and convert to Spark DataFrame\n",
    "excel_dbfs_path = \"/Workspace/Users/rakshit.g@syrencloud.com/employees.xlsx\"  \n",
    "import pandas as pd\n",
    "pdf = pd.read_excel(excel_dbfs_path, sheet_name=0)   \n",
    "print(\"pandas dataframe shape:\", pdf.shape)\n",
    "display(pdf.head(10))\n",
    "\n",
    "# convert to Spark DataFrame (keeps types simpler)\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "print(\"spark schema:\")\n",
    "sdf.printSchema()\n",
    "display(sdf.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8421067-0864-43d8-bda1-7864adf9a6ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning — schema:\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- joining_year: integer (nullable = true)\n |-- bonus_percent: double (nullable = true)\n\n\nSample rows (first 10):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, name: string, department: string, salary: double, joining_year: int, bonus_percent: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL B \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# normalize column names \n",
    "def normalize_column_names(sdf):\n",
    "    for c in sdf.columns:\n",
    "        sdf = sdf.withColumnRenamed(c, c.strip().lower().replace(\" \", \"_\"))\n",
    "    return sdf\n",
    "\n",
    "sdf = normalize_column_names(sdf)\n",
    "\n",
    "# Cast numeric columns to appropriate types (salary & bonus_percent -> double)\n",
    "if 'employee_id' in sdf.columns:\n",
    "    sdf = sdf.withColumn('employee_id', F.col('employee_id').cast('int'))\n",
    "if 'salary' in sdf.columns:\n",
    "    # remove commas/currency if any then cast to double\n",
    "    sdf = sdf.withColumn('salary', F.regexp_replace(F.col('salary').cast('string'), '[,$]', '').cast('double'))\n",
    "if 'joining_year' in sdf.columns:\n",
    "    sdf = sdf.withColumn('joining_year', F.col('joining_year').cast('int'))\n",
    "if 'bonus_percent' in sdf.columns:\n",
    "    sdf = sdf.withColumn('bonus_percent', F.col('bonus_percent').cast('double'))\n",
    "\n",
    "# Trim string columns\n",
    "for name, dtype in sdf.dtypes:\n",
    "    if dtype == 'string':\n",
    "        sdf = sdf.withColumn(name, F.trim(F.col(name)))\n",
    "\n",
    "print(\"After cleaning — schema:\")\n",
    "sdf.printSchema()\n",
    "print(\"\\nSample rows (first 10):\")\n",
    "display(sdf.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871ba23a-5a00-49e3-93c5-5632f3ca0abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns and row count: 7\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[name: string, salary: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL C - select only name and salary\n",
    "sel_name_salary = sdf.select(\"name\", \"salary\")\n",
    "print(\"Selected columns and row count:\", sel_name_salary.count())\n",
    "display(sel_name_salary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6ec81b-8fa9-4177-95a4-ebfece4d09bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered count: 4\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, name: string, department: string, salary: double, joining_year: int, bonus_percent: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL D - filter employees with salary > 60000\n",
    "high_paid = sdf.filter(F.col(\"salary\") > 60000)\n",
    "print(\"Filtered count:\", high_paid.count())\n",
    "display(high_paid.orderBy(F.col(\"salary\").desc()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef167f8b-b5a1-4237-a144-f9f3e2128788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, name: string, department: string, salary: double, bonus_percent: double, bonus_amount: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL E - add bonus_amount and show results (rounded to 2 decimals)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# add bonus_amount column\n",
    "sdf = sdf.withColumn(\"bonus_amount\", F.col(\"salary\") * F.col(\"bonus_percent\") / 100.0)\n",
    "\n",
    "# display selected columns with bonus rounded\n",
    "display(\n",
    "    sdf.select(\n",
    "        \"employee_id\",\n",
    "        \"name\",\n",
    "        \"department\",\n",
    "        \"salary\",\n",
    "        \"bonus_percent\",\n",
    "        F.round(F.col(\"bonus_amount\"), 2).alias(\"bonus_amount\")\n",
    "    ).orderBy(F.col(\"salary\").desc())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb434431-5ab3-492c-8056-6b97944d16c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated schema:\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- year_joined: integer (nullable = true)\n |-- bonus_amount: double (nullable = true)\n\n\nSample rows (first 8):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, name: string, department: string, salary: double, year_joined: int, bonus_amount: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL F - rename joining_year -> year_joined and drop bonus_percent\n",
    "sdf = sdf.withColumnRenamed(\"joining_year\", \"year_joined\")\n",
    "\n",
    "# drop bonus_percent as requested by the assignment\n",
    "if \"bonus_percent\" in sdf.columns:\n",
    "    sdf = sdf.drop(\"bonus_percent\")\n",
    "\n",
    "print(\"Updated schema:\")\n",
    "sdf.printSchema()\n",
    "\n",
    "print(\"\\nSample rows (first 8):\")\n",
    "display(sdf.limit(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cb1fc5-6fc7-4342-88fe-530687702982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Salary per Department:\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|        HR|          51000.0|\n|   Finance|          61000.0|\n|        IT|81666.66666666667|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# CELL G - Average salary per department\n",
    "avg_salary_df = sdf.groupBy(\"department\").avg(\"salary\")\n",
    "\n",
    "print(\"Average Salary per Department:\")\n",
    "avg_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a43550b-02ef-4f16-bc03-36fa564f7c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of employees per department:\n+----------+-------------+\n|department|num_employees|\n+----------+-------------+\n|        HR|            2|\n|   Finance|            2|\n|        IT|            3|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# CELL H: count per department\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "count_dept = sdf.groupBy(\"department\").agg(F.count(\"*\").alias(\"num_employees\"))\n",
    "print(\"Count of employees per department:\")\n",
    "count_dept.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e877317b-7083-4316-af12-3ae93db47ceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum salary: 90000.0\n\nTop employees by salary (desc):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, name: string, department: string, salary: double, year_joined: int, bonus_amount: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEmployees sorted by year_joined (asc):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, name: string, department: string, salary: double, year_joined: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL I - max salary, top by salary (desc), and sort by year_joined (asc)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Maximum salary\n",
    "max_row = sdf.agg(F.max(\"salary\").alias(\"max_salary\")).collect()[0]\n",
    "print(\"Maximum salary:\", max_row[\"max_salary\"])\n",
    "\n",
    "# 2) Top employees by salary (descending)\n",
    "print(\"\\nTop employees by salary (desc):\")\n",
    "display(\n",
    "    sdf.orderBy(F.col(\"salary\").desc())\n",
    "       .select(\"employee_id\", \"name\", \"department\", \"salary\", \"year_joined\", \"bonus_amount\")\n",
    "       .limit(10)\n",
    ")\n",
    "\n",
    "# 3) Employees sorted by year_joined (ascending)\n",
    "if \"year_joined\" in sdf.columns:\n",
    "    print(\"\\nEmployees sorted by year_joined (asc):\")\n",
    "    display(\n",
    "        sdf.orderBy(F.col(\"year_joined\").asc())\n",
    "           .select(\"employee_id\", \"name\", \"department\", \"salary\", \"year_joined\")\n",
    "           .limit(20)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo year_joined column found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35dffc9f-84e6-4c25-980f-53151243c873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dept table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[department: string, dept_name: string, location: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined sample (first 20 rows):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[department: string, employee_id: int, name: string, salary: double, year_joined: int, bonus_amount: double, dept_name: string, location: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# JOIN CELL J - create dept details and left-join with employees\n",
    "dept_data = [(\"HR\",\"Human Resources\",\"New York\"),\n",
    "             (\"Finance\",\"Finance Dept\",\"London\"),\n",
    "             (\"IT\",\"Information Technology\",\"San Francisco\")]\n",
    "dept_cols = [\"department\",\"dept_name\",\"location\"]\n",
    "\n",
    "dept_df = spark.createDataFrame(dept_data, schema=dept_cols)\n",
    "print(\"Dept table:\")\n",
    "display(dept_df)\n",
    "\n",
    "joined = sdf.join(dept_df, on=\"department\", how=\"left\")\n",
    "print(\"Joined sample (first 20 rows):\")\n",
    "display(joined.orderBy(\"employee_id\").limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abae2e0-48a4-4260-bbb4-b0830d3c0693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined schema:\nroot\n |-- department: string (nullable = true)\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- year_joined: integer (nullable = true)\n |-- bonus_amount: double (nullable = true)\n |-- dept_name: string (nullable = true)\n |-- location: string (nullable = true)\n\n\nFirst 20 joined rows:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[department: string, employee_id: int, name: string, salary: double, year_joined: int, bonus_amount: double, dept_name: string, location: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFirst 5 rows (explicit):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[department: string, employee_id: int, name: string, salary: double, year_joined: int, bonus_amount: double, dept_name: string, location: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTotal rows in joined DF: 7\n\nCollected sample (first 5 shown):\nRow(department='HR', employee_id=101, name='Alice', salary=50000.0, year_joined=2018, bonus_amount=2500.0, dept_name='Human Resources', location='New York')\nRow(department='Finance', employee_id=102, name='Bob', salary=60000.0, year_joined=2019, bonus_amount=4200.0, dept_name='Finance Dept', location='London')\nRow(department='IT', employee_id=103, name='Charlie', salary=75000.0, year_joined=2017, bonus_amount=7500.0, dept_name='Information Technology', location='San Francisco')\nRow(department='Finance', employee_id=104, name='David', salary=62000.0, year_joined=2020, bonus_amount=3720.0, dept_name='Finance Dept', location='London')\nRow(department='IT', employee_id=105, name='Eva', salary=80000.0, year_joined=2021, bonus_amount=9600.0, dept_name='Information Technology', location='San Francisco')\n\nTake first 3 (take):\nRow(department='HR', employee_id=101, name='Alice', salary=50000.0, year_joined=2018, bonus_amount=2500.0, dept_name='Human Resources', location='New York')\nRow(department='Finance', employee_id=102, name='Bob', salary=60000.0, year_joined=2019, bonus_amount=4200.0, dept_name='Finance Dept', location='London')\nRow(department='IT', employee_id=103, name='Charlie', salary=75000.0, year_joined=2017, bonus_amount=7500.0, dept_name='Information Technology', location='San Francisco')\n"
     ]
    }
   ],
   "source": [
    "# Cell K\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "print(\"Joined schema:\")\n",
    "joined.printSchema()\n",
    "\n",
    "print(\"\\nFirst 20 joined rows:\")\n",
    "display(joined.orderBy(\"employee_id\").limit(20))\n",
    "\n",
    "print(\"\\nFirst 5 rows (explicit):\")\n",
    "display(joined.limit(5))\n",
    "\n",
    "# total count\n",
    "total = joined.count()\n",
    "print(f\"\\nTotal rows in joined DF: {total}\")\n",
    "\n",
    "\n",
    "rows = joined.collect()\n",
    "print(\"\\nCollected sample (first 5 shown):\")\n",
    "for r in rows[:5]:\n",
    "    print(r)\n",
    "\n",
    "print(\"\\nTake first 3 (take):\")\n",
    "for r in joined.take(3):\n",
    "    print(r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2cc54b-0deb-4cb5-9492-2f0601d24bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<a href=\"data:text/csv;base64,ZGVwYXJ0bWVudCxlbXBsb3llZV9pZCxuYW1lLHNhbGFyeSx5ZWFyX2pvaW5lZCxib251c19hbW91bnQsZGVwdF9uYW1lLGxvY2F0aW9uCkhSLDEwMSxBbGljZSw1MDAwMC4wLDIwMTgsMjUwMC4wLEh1bWFuIFJlc291cmNlcyxOZXcgWW9yawpGaW5hbmNlLDEwMixCb2IsNjAwMDAuMCwyMDE5LDQyMDAuMCxGaW5hbmNlIERlcHQsTG9uZG9uCklULDEwMyxDaGFybGllLDc1MDAwLjAsMjAxNyw3NTAwLjAsSW5mb3JtYXRpb24gVGVjaG5vbG9neSxTYW4gRnJhbmNpc2NvCkZpbmFuY2UsMTA0LERhdmlkLDYyMDAwLjAsMjAyMCwzNzIwLjAsRmluYW5jZSBEZXB0LExvbmRvbgpJVCwxMDUsRXZhLDgwMDAwLjAsMjAyMSw5NjAwLjAsSW5mb3JtYXRpb24gVGVjaG5vbG9neSxTYW4gRnJhbmNpc2NvCkhSLDEwNixGcmFuayw1MjAwMC4wLDIwMTgsMjYwMC4wLEh1bWFuIFJlc291cmNlcyxOZXcgWW9yawpJVCwxMDcsR3JhY2UsOTAwMDAuMCwyMDE5LDEzNTAwLjAsSW5mb3JtYXRpb24gVGVjaG5vbG9neSxTYW4gRnJhbmNpc2NvCg==\" download=\"employees_with_dept.csv\">Download employees_with_dept.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<a href=\"data:text/csv;base64,ZW1wbG95ZWVfaWQsbmFtZSxkZXBhcnRtZW50LHNhbGFyeSx5ZWFyX2pvaW5lZCxib251c19hbW91bnQKMTAxLEFsaWNlLEhSLDUwMDAwLjAsMjAxOCwyNTAwLjAKMTAyLEJvYixGaW5hbmNlLDYwMDAwLjAsMjAxOSw0MjAwLjAKMTAzLENoYXJsaWUsSVQsNzUwMDAuMCwyMDE3LDc1MDAuMAoxMDQsRGF2aWQsRmluYW5jZSw2MjAwMC4wLDIwMjAsMzcyMC4wCjEwNSxFdmEsSVQsODAwMDAuMCwyMDIxLDk2MDAuMAoxMDYsRnJhbmssSFIsNTIwMDAuMCwyMDE4LDI2MDAuMAoxMDcsR3JhY2UsSVQsOTAwMDAuMCwyMDE5LDEzNTAwLjAK\" download=\"cleaned_employee.csv\">Download cleaned_employee.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<a href=\"data:text/csv;base64,ZGVwYXJ0bWVudCxhdmcoc2FsYXJ5KQpIUiw1MTAwMC4wCkZpbmFuY2UsNjEwMDAuMApJVCw4MTY2Ni42NjY2NjY2NjY2Nwo=\" download=\"avg_salary_by_dept.csv\">Download avg_salary_by_dept.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<a href=\"data:text/csv;base64,ZGVwYXJ0bWVudCxudW1fZW1wbG95ZWVzCkhSLDIKRmluYW5jZSwyCklULDMK\" download=\"count_by_dept.csv\">Download count_by_dept.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL L Download links for small outputs (run in Databricks notebook)\n",
    "import base64, io\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def make_download_link_from_pdf(df, filename):\n",
    "    csv = df.to_csv(index=False)\n",
    "    b64 = base64.b64encode(csv.encode()).decode()\n",
    "    href = f'<a href=\"data:text/csv;base64,{b64}\" download=\"{filename}\">Download {filename}</a>'\n",
    "    display(HTML(href))\n",
    "\n",
    "# joined and cleaned DataFrames should exist in the notebook as Spark DataFrames:\n",
    "# convert to pandas \n",
    "pdf_joined = joined.toPandas()\n",
    "pdf_clean = sdf.toPandas()\n",
    "\n",
    "make_download_link_from_pdf(pdf_joined, \"employees_with_dept.csv\")\n",
    "make_download_link_from_pdf(pdf_clean, \"cleaned_employee.csv\")\n",
    "\n",
    "# optional: aggregates\n",
    "if 'avg_salary_df' in globals():\n",
    "    make_download_link_from_pdf(avg_salary_df.toPandas(), \"avg_salary_by_dept.csv\")\n",
    "if 'count_dept' in globals():\n",
    "    make_download_link_from_pdf(count_dept.toPandas(), \"count_by_dept.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12e37c0-d329-417b-b0d8-5fc5a0d5a7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5372983276421612>, line 21\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# 1) check core DF 'sdf' exists\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m():\n",
       "\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpark DataFrame \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not found in this session. Please re-run the cell that loads employees.xlsx into \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m found. Row count (approx):\u001B[39m\u001B[38;5;124m\"\u001B[39m, sdf\u001B[38;5;241m.\u001B[39mcount())\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# 2) recreate 'joined' if missing\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: Spark DataFrame 'sdf' not found in this session. Please re-run the cell that loads employees.xlsx into 'sdf' first."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RuntimeError",
        "evalue": "Spark DataFrame 'sdf' not found in this session. Please re-run the cell that loads employees.xlsx into 'sdf' first."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>RuntimeError</span>: Spark DataFrame 'sdf' not found in this session. Please re-run the cell that loads employees.xlsx into 'sdf' first."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
        "File \u001B[0;32m<command-5372983276421612>, line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# 1) check core DF 'sdf' exists\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m():\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpark DataFrame \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not found in this session. Please re-run the cell that loads employees.xlsx into \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msdf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m found. Row count (approx):\u001B[39m\u001B[38;5;124m\"\u001B[39m, sdf\u001B[38;5;241m.\u001B[39mcount())\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# 2) recreate 'joined' if missing\u001B[39;00m\n",
        "\u001B[0;31mRuntimeError\u001B[0m: Spark DataFrame 'sdf' not found in this session. Please re-run the cell that loads employees.xlsx into 'sdf' first."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-09-02 04:13:44",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}